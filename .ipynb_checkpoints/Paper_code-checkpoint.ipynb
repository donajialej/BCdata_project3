{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ALEJANDRA: just copy paste of code from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import six\n",
    "import marshal\n",
    "import types as python_types\n",
    "\n",
    "\n",
    "def get_from_module(identifier, module_params, module_name,\n",
    "                    instantiate=False, kwargs=None):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = module_params.get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid ' + str(module_name) + ': ' +\n",
    "                             str(identifier))\n",
    "        if instantiate and not kwargs:\n",
    "            return res()\n",
    "        elif instantiate and kwargs:\n",
    "            return res(**kwargs)\n",
    "        else:\n",
    "            return res\n",
    "    elif isinstance(identifier, dict):\n",
    "        name = identifier.pop('name')\n",
    "        res = module_params.get(name)\n",
    "        if res:\n",
    "            return res(**identifier)\n",
    "        else:\n",
    "            raise ValueError('Invalid ' + str(module_name) + ': ' +\n",
    "                             str(identifier))\n",
    "    return identifier\n",
    "\n",
    "\n",
    "def make_tuple(*args):\n",
    "    return args\n",
    "\n",
    "\n",
    "def func_dump(func):\n",
    "    '''Serialize user defined function.'''\n",
    "    code = marshal.dumps(func.__code__).decode('raw_unicode_escape')\n",
    "    defaults = func.__defaults__\n",
    "    if func.__closure__:\n",
    "        closure = tuple(c.cell_contents for c in func.__closure__)\n",
    "    else:\n",
    "        closure = None\n",
    "    return code, defaults, closure\n",
    "\n",
    "\n",
    "def func_load(code, defaults=None, closure=None, globs=None):\n",
    "    '''Deserialize user defined function.'''\n",
    "    if isinstance(code, (tuple, list)):  # unpack previous dump\n",
    "        code, defaults, closure = code\n",
    "    code = marshal.loads(code.encode('raw_unicode_escape'))\n",
    "    if globs is None:\n",
    "        globs = globals()\n",
    "    return python_types.FunctionType(code, globs,\n",
    "                                     name=code.co_name,\n",
    "                                     argdefs=defaults,\n",
    "                                     closure=closure)\n",
    "\n",
    "\n",
    "class Progbar(object):\n",
    "\n",
    "    def __init__(self, target, width=30, verbose=1, interval=0.01):\n",
    "        '''Dislays a progress bar.\n",
    "        # Arguments:\n",
    "            target: Total number of steps expected.\n",
    "            interval: Minimum visual progress update interval (in seconds).\n",
    "        '''\n",
    "        self.width = width\n",
    "        self.target = target\n",
    "        self.sum_values = {}\n",
    "        self.unique_values = []\n",
    "        self.start = time.time()\n",
    "        self.last_update = 0\n",
    "        self.interval = interval\n",
    "        self.total_width = 0\n",
    "        self.seen_so_far = 0\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def update(self, current, values=[], force=False):\n",
    "        '''Updates the progress bar.\n",
    "        # Arguments\n",
    "            current: Index of current step.\n",
    "            values: List of tuples (name, value_for_last_step).\n",
    "                The progress bar will display averages for these values.\n",
    "            force: Whether to force visual progress update.\n",
    "        '''\n",
    "        for k, v in values:\n",
    "            if k not in self.sum_values:\n",
    "                self.sum_values[k] = [v * (current - self.seen_so_far),\n",
    "                                      current - self.seen_so_far]\n",
    "                self.unique_values.append(k)\n",
    "            else:\n",
    "                self.sum_values[k][0] += v * (current - self.seen_so_far)\n",
    "                self.sum_values[k][1] += (current - self.seen_so_far)\n",
    "        self.seen_so_far = current\n",
    "\n",
    "        now = time.time()\n",
    "        if self.verbose == 1:\n",
    "            if not force and (now - self.last_update) < self.interval:\n",
    "                return\n",
    "\n",
    "            prev_total_width = self.total_width\n",
    "            sys.stdout.write('\\b' * prev_total_width)\n",
    "            sys.stdout.write('\\r')\n",
    "\n",
    "            numdigits = int(np.floor(np.log10(self.target))) + 1\n",
    "            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n",
    "            bar = barstr % (current, self.target)\n",
    "            prog = float(current) / self.target\n",
    "            prog_width = int(self.width * prog)\n",
    "            if prog_width > 0:\n",
    "                bar += ('=' * (prog_width-1))\n",
    "                if current < self.target:\n",
    "                    bar += '>'\n",
    "                else:\n",
    "                    bar += '='\n",
    "            bar += ('.' * (self.width - prog_width))\n",
    "            bar += ']'\n",
    "            sys.stdout.write(bar)\n",
    "            self.total_width = len(bar)\n",
    "\n",
    "            if current:\n",
    "                time_per_unit = (now - self.start) / current\n",
    "            else:\n",
    "                time_per_unit = 0\n",
    "            eta = time_per_unit * (self.target - current)\n",
    "            info = ''\n",
    "            if current < self.target:\n",
    "                info += ' - ETA: %ds' % eta\n",
    "            else:\n",
    "                info += ' - %ds' % (now - self.start)\n",
    "            for k in self.unique_values:\n",
    "                info += ' - %s:' % k\n",
    "                if isinstance(self.sum_values[k], list):\n",
    "                    avg = self.sum_values[k][0] / max(1, self.sum_values[k][1])\n",
    "                    if abs(avg) > 1e-3:\n",
    "                        info += ' %.4f' % avg\n",
    "                    else:\n",
    "                        info += ' %.4e' % avg\n",
    "                else:\n",
    "                    info += ' %s' % self.sum_values[k]\n",
    "\n",
    "            self.total_width += len(info)\n",
    "            if prev_total_width > self.total_width:\n",
    "                info += ((prev_total_width - self.total_width) * ' ')\n",
    "\n",
    "            sys.stdout.write(info)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if current >= self.target:\n",
    "                sys.stdout.write('\\n')\n",
    "\n",
    "        if self.verbose == 2:\n",
    "            if current >= self.target:\n",
    "                info = '%ds' % (now - self.start)\n",
    "                for k in self.unique_values:\n",
    "                    info += ' - %s:' % k\n",
    "                    avg = self.sum_values[k][0] / max(1, self.sum_values[k][1])\n",
    "                    if avg > 1e-3:\n",
    "                        info += ' %.4f' % avg\n",
    "                    else:\n",
    "                        info += ' %.4e' % avg\n",
    "                sys.stdout.write(info + \"\\n\")\n",
    "\n",
    "        self.last_update = now\n",
    "\n",
    "    def add(self, n, values=[]):\n",
    "        self.update(self.seen_so_far + n, values)\n",
    "\n",
    "\n",
    "def display_table(rows, positions):\n",
    "\n",
    "    def display_row(objects, positions):\n",
    "        line = ''\n",
    "        for i in range(len(objects)):\n",
    "            line += str(objects[i])\n",
    "            line = line[:positions[i]]\n",
    "            line += ' ' * (positions[i] - len(line))\n",
    "        print(line)\n",
    "\n",
    "    for objects in rows:\n",
    "        display_row(objects, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializations\n",
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "#from keras.utils.generic_utils import get_from_module\n",
    "\n",
    "\n",
    "def get_fans(shape, dim_ordering='th'):\n",
    "    if len(shape) == 2:\n",
    "        fan_in = shape[0]\n",
    "        fan_out = shape[1]\n",
    "    elif len(shape) == 4 or len(shape) == 5:\n",
    "        # assuming convolution kernels (2D or 3D).\n",
    "        # TH kernel shape: (depth, input_depth, ...)\n",
    "        # TF kernel shape: (..., input_depth, depth)\n",
    "        if dim_ordering == 'th':\n",
    "            receptive_field_size = np.prod(shape[2:])\n",
    "            fan_in = shape[1] * receptive_field_size\n",
    "            fan_out = shape[0] * receptive_field_size\n",
    "        elif dim_ordering == 'tf':\n",
    "            receptive_field_size = np.prod(shape[:2])\n",
    "            fan_in = shape[-2] * receptive_field_size\n",
    "            fan_out = shape[-1] * receptive_field_size\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering: ' + dim_ordering)\n",
    "    else:\n",
    "        # no specific assumptions\n",
    "        fan_in = np.sqrt(np.prod(shape))\n",
    "        fan_out = np.sqrt(np.prod(shape))\n",
    "    return fan_in, fan_out\n",
    "\n",
    "\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    return K.random_uniform_variable(shape, -scale, scale, name=name)\n",
    "\n",
    "\n",
    "def normal(shape, scale=0.05, name=None):\n",
    "    return K.random_normal_variable(shape, 0.0, scale, name=name)\n",
    "\n",
    "\n",
    "def lecun_uniform(shape, name=None, dim_ordering='th'):\n",
    "    ''' Reference: LeCun 98, Efficient Backprop\n",
    "        http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "    '''\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    scale = np.sqrt(3. / fan_in)\n",
    "    return uniform(shape, scale, name=name)\n",
    "\n",
    "\n",
    "def glorot_normal(shape, name=None, dim_ordering='th'):\n",
    "    ''' Reference: Glorot & Bengio, AISTATS 2010\n",
    "    '''\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(2. / (fan_in + fan_out))\n",
    "    return normal(shape, s, name=name)\n",
    "\n",
    "\n",
    "def glorot_uniform(shape, name=None, dim_ordering='th'):\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(6. / (fan_in + fan_out))\n",
    "    return uniform(shape, s, name=name)\n",
    "\n",
    "\n",
    "def fanintimesfanouttimesfanout(shape, name=None, dim_ordering='th'):\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(6. / (fan_in*fan_out*fan_out))\n",
    "    return uniform(shape, s, name=name)\n",
    "\n",
    "\n",
    "def fanintimesfanout(shape, name=None, dim_ordering='th'):\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(6. / (fan_in*fan_out))\n",
    "    return uniform(shape, s, name=name)\n",
    "\n",
    "\n",
    "def fanintimesfanouttimestwo(shape, name=None, dim_ordering='th'):\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(6. / (fan_in*fan_out*2))\n",
    "    return uniform(shape, s, name=name)\n",
    "\n",
    "\n",
    "def fanintimesfanouttimesfour(shape, name=None, dim_ordering='th'):\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(6. / (fan_in*fan_out*4))\n",
    "    return uniform(shape, s, name=name)\n",
    "\n",
    "\n",
    "def he_normal(shape, name=None, dim_ordering='th'):\n",
    "    ''' Reference:  He et al., http://arxiv.org/abs/1502.01852\n",
    "    '''\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(2. / fan_in)\n",
    "    return normal(shape, s, name=name)\n",
    "\n",
    "\n",
    "def he_uniform(shape, name=None, dim_ordering='th'):\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(6. / fan_in)\n",
    "    return uniform(shape, s, name=name)\n",
    "\n",
    "\n",
    "def orthogonal(shape, scale=1.1, name=None):\n",
    "    ''' From Lasagne. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n",
    "    '''\n",
    "    flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "    a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    # pick the one with the correct shape\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.reshape(shape)\n",
    "    return K.variable(scale * q[:shape[0], :shape[1]], name=name)\n",
    "\n",
    "\n",
    "def identity(shape, scale=1, name=None):\n",
    "    if len(shape) != 2 or shape[0] != shape[1]:\n",
    "        raise ValueError('Identity matrix initialization can only be used '\n",
    "                         'for 2D square matrices.')\n",
    "    else:\n",
    "        return K.variable(scale * np.identity(shape[0]), name=name)\n",
    "\n",
    "\n",
    "def zero(shape, name=None):\n",
    "    return K.zeros(shape, name=name)\n",
    "\n",
    "\n",
    "def one(shape, name=None):\n",
    "    return K.ones(shape, name=name)\n",
    "\n",
    "\n",
    "def get(identifier, **kwargs):\n",
    "    return get_from_module(identifier, globals(),\n",
    "                           'initialization', kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regularizers\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from keras import backend as K\n",
    "#from .utils.generic_utils import get_from_module\n",
    "import warnings\n",
    "\n",
    "\n",
    "class Regularizer(object):\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return 0\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'name': self.__class__.__name__}\n",
    "\n",
    "    def set_param(self, _):\n",
    "        warnings.warn('The `set_param` method on regularizers is deprecated. '\n",
    "                      'It no longer does anything, '\n",
    "                      'and it will be removed after 06/2017.')\n",
    "\n",
    "    def set_layer(self, _):\n",
    "        warnings.warn('The `set_layer` method on regularizers is deprecated. '\n",
    "                      'It no longer does anything, '\n",
    "                      'and it will be removed after 06/2017.')\n",
    "\n",
    "\n",
    "class SmoothnessRegularizer(Regularizer):\n",
    "\n",
    "    def __init__(self, smoothness):\n",
    "        self.smoothness = smoothness\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return K.mean(K.abs(x[1:, :]-x[:-1,:]))*self.smoothness\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'name': self.__class__.__name__,\n",
    "                'smoothness': float(self.smoothness)}\n",
    "\n",
    "\n",
    "\n",
    "class EigenvalueRegularizer(Regularizer):\n",
    "    '''This takes a constant that controls\n",
    "    the regularization by Eigenvalue Decay on the\n",
    "    current layer and outputs the regularized\n",
    "    loss (evaluated on the training data) and\n",
    "    the original loss (evaluated on the\n",
    "    validation data).\n",
    "    '''\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if K.ndim(x) != 2:\n",
    "            raise ValueError('EigenvalueRegularizer '\n",
    "                             'is only available for tensors of rank 2.')\n",
    "        covariance = K.dot(K.transpose(x), x)\n",
    "        dim1, dim2 = K.eval(K.shape(covariance))\n",
    "\n",
    "        # Power method for approximating the dominant eigenvector:\n",
    "        power = 9  # Number of iterations of the power method.\n",
    "        o = K.ones([dim1, 1])  # Initial values for the dominant eigenvector.\n",
    "        main_eigenvect = K.dot(covariance, o)\n",
    "        for n in range(power - 1):\n",
    "            main_eigenvect = K.dot(covariance, main_eigenvect)\n",
    "        covariance_d = K.dot(covariance, main_eigenvect)\n",
    "\n",
    "        # The corresponding dominant eigenvalue:\n",
    "        main_eigenval = (K.dot(K.transpose(covariance_d), main_eigenvect) /\n",
    "                         K.dot(K.transpose(main_eigenvect), main_eigenvect))\n",
    "        # Multiply by the given regularization gain.\n",
    "        regularization = (main_eigenval ** 0.5) * self.k\n",
    "        return K.sum(regularization)\n",
    "\n",
    "\n",
    "class L1L2Regularizer(Regularizer):\n",
    "\n",
    "    def __init__(self, l1=0., l2=0.):\n",
    "        self.l1 = K.cast_to_floatx(l1)\n",
    "        self.l2 = K.cast_to_floatx(l2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        regularization = 0\n",
    "        if self.l1:\n",
    "            regularization += K.sum(self.l1 * K.abs(x))\n",
    "        if self.l2:\n",
    "            regularization += K.sum(self.l2 * K.square(x))\n",
    "        return regularization\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'name': self.__class__.__name__,\n",
    "                'l1': float(self.l1),\n",
    "                'l2': float(self.l2)}\n",
    "\n",
    "\n",
    "# Aliases.\n",
    "\n",
    "WeightRegularizer = L1L2Regularizer\n",
    "ActivityRegularizer = L1L2Regularizer\n",
    "\n",
    "\n",
    "def l1(l=0.01):\n",
    "    return L1L2Regularizer(l1=l)\n",
    "\n",
    "\n",
    "def l2(l=0.01):\n",
    "    return L1L2Regularizer(l2=l)\n",
    "\n",
    "\n",
    "def l1l2(l1=0.01, l2=0.01):\n",
    "    return L1L2Regularizer(l1=l1, l2=l2)\n",
    "\n",
    "\n",
    "def activity_l1(l=0.01):\n",
    "    return L1L2Regularizer(l1=l)\n",
    "\n",
    "\n",
    "def activity_l2(l=0.01):\n",
    "    return L1L2Regularizer(l2=l)\n",
    "\n",
    "\n",
    "def activity_l1l2(l1=0.01, l2=0.01):\n",
    "    return L1L2Regularizer(l1=l1, l2=l2)\n",
    "\n",
    "\n",
    "def get(identifier, kwargs=None):\n",
    "    return get_from_module(identifier, globals(), 'regularizer',\n",
    "                           instantiate=True, kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization.py\n",
    "\n",
    "from keras.engine import Layer, InputSpec\n",
    "#from keras import initializations, regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    '''Normalize the activations of the previous layer at each batch,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "    # Arguments\n",
    "        epsilon: small float > 0. Fuzz parameter.\n",
    "            Theano expects epsilon >= 1e-5.\n",
    "        mode: integer, 0, 1 or 2.\n",
    "            - 0: feature-wise normalization.\n",
    "                Each feature map in the input will\n",
    "                be normalized separately. The axis on which\n",
    "                to normalize is specified by the `axis` argument.\n",
    "                Note that if the input is a 4D image tensor\n",
    "                using Theano conventions (samples, channels, rows, cols)\n",
    "                then you should set `axis` to `1` to normalize along\n",
    "                the channels axis.\n",
    "                During training we use per-batch statistics to normalize\n",
    "                the data, and during testing we use running averages\n",
    "                computed during the training phase.\n",
    "            - 1: sample-wise normalization. This mode assumes a 2D input.\n",
    "            - 2: feature-wise normalization, like mode 0, but\n",
    "                using per-batch statistics to normalize the data during both\n",
    "                testing and training.\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "            Note that the order of this list is [gamma, beta, mean, std]\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the gamma vector.\n",
    "        beta_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the beta vector.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf)\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-3, mode=0, axis=-1, momentum=0.99,\n",
    "                 weights=None, beta_init='zero', gamma_init='one',\n",
    "                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.epsilon = epsilon\n",
    "        self.mode = mode\n",
    "        self.axis = axis\n",
    "        self.momentum = momentum\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.initial_weights = weights\n",
    "        if self.mode == 0:\n",
    "            self.uses_learning_phase = True\n",
    "        super(BatchNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis],)\n",
    "\n",
    "        self.gamma = self.add_weight(shape,\n",
    "                                     initializer=self.gamma_init,\n",
    "                                     regularizer=self.gamma_regularizer,\n",
    "                                     name='{}_gamma'.format(self.name))\n",
    "        self.beta = self.add_weight(shape,\n",
    "                                    initializer=self.beta_init,\n",
    "                                    regularizer=self.beta_regularizer,\n",
    "                                    name='{}_beta'.format(self.name))\n",
    "        self.running_mean = self.add_weight(shape, initializer='zero',\n",
    "                                            name='{}_running_mean'.format(self.name),\n",
    "                                            trainable=False)\n",
    "        self.running_std = self.add_weight(shape, initializer='one',\n",
    "                                           name='{}_running_std'.format(self.name),\n",
    "                                           trainable=False)\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.mode == 0 or self.mode == 2:\n",
    "            assert self.built, 'Layer must be built before being called'\n",
    "            input_shape = K.int_shape(x)\n",
    "\n",
    "            reduction_axes = list(range(len(input_shape)))\n",
    "            del reduction_axes[self.axis]\n",
    "            broadcast_shape = [1] * len(input_shape)\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "            x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                x, self.gamma, self.beta, reduction_axes,\n",
    "                epsilon=self.epsilon)\n",
    "\n",
    "            if self.mode == 0:\n",
    "                self.add_update([K.moving_average_update(self.running_mean, mean, self.momentum),\n",
    "                                 K.moving_average_update(self.running_std, std, self.momentum)], x)\n",
    "\n",
    "                if sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, self.running_mean, self.running_std,\n",
    "                        self.beta, self.gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "                else:\n",
    "                    # need broadcasting\n",
    "                    broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n",
    "                    broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n",
    "                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                    broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, broadcast_running_mean, broadcast_running_std,\n",
    "                        broadcast_beta, broadcast_gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "\n",
    "                # pick the normalized form of x corresponding to the training phase\n",
    "                x_normed = K.in_train_phase(x_normed, x_normed_running)\n",
    "\n",
    "        elif self.mode == 1:\n",
    "            # sample-wise normalization\n",
    "            m = K.mean(x, axis=-1, keepdims=True)\n",
    "            std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n",
    "            x_normed = (x - m) / (std + self.epsilon)\n",
    "            x_normed = self.gamma * x_normed + self.beta\n",
    "        return x_normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'epsilon': self.epsilon,\n",
    "                  'mode': self.mode,\n",
    "                  'axis': self.axis,\n",
    "                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n",
    "                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None,\n",
    "                  'momentum': self.momentum}\n",
    "        base_config = super(BatchNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class RevCompConv1DBatchNorm(Layer):\n",
    "    '''Batch norm that shares weights over reverse complement channels\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-3, mode=0, axis=-1, momentum=0.99,\n",
    "                 weights=None, beta_init='zero', gamma_init='one',\n",
    "                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.epsilon = epsilon\n",
    "        self.mode = mode\n",
    "        assert axis==-1 or axis==2, \"Intended for Conv1D\"\n",
    "        self.axis = axis\n",
    "        self.momentum = momentum\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.initial_weights = weights\n",
    "        if self.mode == 0:\n",
    "            self.uses_learning_phase = True\n",
    "        super(RevCompConv1DBatchNorm, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.num_input_chan = input_shape[self.axis]\n",
    "        self.input_len = input_shape[1]\n",
    "        assert len(input_shape)==3,\\\n",
    "         \"Implementation done with RevCompConv1D input in mind\"\n",
    "        assert self.input_len is not None,\\\n",
    "         \"not implemented for undefined input len\"\n",
    "        assert self.num_input_chan%2 == 0, \"should be even for revcomp input\"\n",
    "        shape = (int(self.num_input_chan/2),)\n",
    "\n",
    "        self.gamma = self.add_weight(shape,\n",
    "                                     initializer=self.gamma_init,\n",
    "                                     regularizer=self.gamma_regularizer,\n",
    "                                     name='{}_gamma'.format(self.name))\n",
    "        self.beta = self.add_weight(shape,\n",
    "                                    initializer=self.beta_init,\n",
    "                                    regularizer=self.beta_regularizer,\n",
    "                                    name='{}_beta'.format(self.name))\n",
    "        self.running_mean = self.add_weight(shape, initializer='zero',\n",
    "                                            name='{}_running_mean'.format(self.name),\n",
    "                                            trainable=False)\n",
    "        self.running_std = self.add_weight(shape, initializer='one',\n",
    "                                           name='{}_running_std'.format(self.name),\n",
    "                                           trainable=False)\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        orig_x = x\n",
    "        #create a fake x by concatentating reverse-complemented pairs\n",
    "        #along the length dimension\n",
    "        x = K.concatenate(\n",
    "            tensors=[x[:,:,:int(self.num_input_chan/2)],\n",
    "                     x[:,:,int(self.num_input_chan/2):][:,:,::-1]],\n",
    "            axis=1)\n",
    "        if self.mode == 0 or self.mode == 2:\n",
    "            assert self.built, 'Layer must be built before being called'\n",
    "\n",
    "            reduction_axes = list(range(3))\n",
    "            del reduction_axes[self.axis]\n",
    "            broadcast_shape = [1] * 3\n",
    "            broadcast_shape[self.axis] = int(self.num_input_chan/2)\n",
    "\n",
    "            x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                x, self.gamma, self.beta, reduction_axes,\n",
    "                epsilon=self.epsilon)\n",
    "\n",
    "            if self.mode == 0:\n",
    "                self.add_update([K.moving_average_update(self.running_mean, mean, self.momentum),\n",
    "                                 K.moving_average_update(self.running_std, std, self.momentum)], x)\n",
    "\n",
    "                # need broadcasting\n",
    "                broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n",
    "                broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n",
    "                broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "                x_normed_running = K.batch_normalization(\n",
    "                    x, broadcast_running_mean, broadcast_running_std,\n",
    "                    broadcast_beta, broadcast_gamma,\n",
    "                    epsilon=self.epsilon)\n",
    "\n",
    "                # pick the normalized form of x corresponding to the training phase\n",
    "                x_normed = K.in_train_phase(x_normed, x_normed_running)\n",
    "\n",
    "        elif self.mode == 1:\n",
    "            # sample-wise normalization\n",
    "            m = K.mean(x, axis=-1, keepdims=True)\n",
    "            std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n",
    "            x_normed = (x - m) / (std + self.epsilon)\n",
    "            x_normed = self.gamma * x_normed + self.beta\n",
    "        #recover the reverse-complemented channels\n",
    "        true_x_normed = K.concatenate(\n",
    "            tensors=[x_normed[:,:self.input_len,:],\n",
    "                     x_normed[:,self.input_len:,:][:,:,::-1]],\n",
    "            axis=2)\n",
    "        return true_x_normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'epsilon': self.epsilon,\n",
    "                  'mode': self.mode,\n",
    "                  'axis': self.axis,\n",
    "                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n",
    "                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None,\n",
    "                  'momentum': self.momentum}\n",
    "        base_config = super(RevCompConv1DBatchNorm, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "import functools\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import activations, initializations, regularizers, constraints\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras.utils.np_utils import conv_output_length, conv_input_length\n",
    "\n",
    "# imports for backwards namespace compatibility\n",
    "from keras.pooling import AveragePooling1D, AveragePooling2D, AveragePooling3D\n",
    "from keras.pooling import MaxPooling1D, MaxPooling2D, MaxPooling3D\n",
    "\n",
    "\n",
    "class Convolution1D(Layer):\n",
    "    '''Convolution operator for filtering neighborhoods of one-dimensional inputs.\n",
    "    When using this layer as the first layer in a model,\n",
    "    either provide the keyword argument `input_dim`\n",
    "    (int, e.g. 128 for sequences of 128-dimensional vectors),\n",
    "    or `input_shape` (tuple of integers, e.g. (10, 128) for sequences\n",
    "    of 10 vectors of 128-dimensional vectors).\n",
    "    # Example\n",
    "    ```python\n",
    "        # apply a convolution 1d of length 3 to a sequence with 10 timesteps,\n",
    "        # with 64 output filters\n",
    "        model = Sequential()\n",
    "        model.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))\n",
    "        # now model.output_shape == (None, 10, 64)\n",
    "        # add a new conv1d on top\n",
    "        model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "        # now model.output_shape == (None, 10, 32)\n",
    "    ```\n",
    "    # Arguments\n",
    "        nb_filter: Number of convolution kernels to use\n",
    "            (dimensionality of the output).\n",
    "        filter_length: The extension (spatial or temporal) of each filter.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)),\n",
    "            or alternatively, Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n",
    "        subsample_length: factor by which to subsample output.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        bias: whether to include a bias\n",
    "            (i.e. make the layer affine rather than linear).\n",
    "        input_dim: Number of channels/dimensions in the input.\n",
    "            Either this argument or the keyword argument `input_shape`must be\n",
    "            provided when using this layer as the first layer in a model.\n",
    "        input_length: Length of input sequences, when it is constant.\n",
    "            This argument is required if you are going to connect\n",
    "            `Flatten` then `Dense` layers upstream\n",
    "            (without it, the shape of the dense outputs cannot be computed).\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, input_dim)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, new_steps, nb_filter)`.\n",
    "        `steps` value might have changed due to padding.\n",
    "    '''\n",
    "    def __init__(self, nb_filter, filter_length,\n",
    "                 init='glorot_uniform', activation=None, weights=None,\n",
    "                 border_mode='valid', subsample_length=1,\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, input_dim=None, input_length=None, **kwargs):\n",
    "\n",
    "        if border_mode not in {'valid', 'same', 'full'}:\n",
    "            raise ValueError('Invalid border mode for Convolution1D:', border_mode)\n",
    "        self.nb_filter = nb_filter\n",
    "        self.filter_length = filter_length\n",
    "        self.init = initializations.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.border_mode = border_mode\n",
    "        self.subsample_length = subsample_length\n",
    "\n",
    "        self.subsample = (subsample_length, 1)\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        self.initial_weights = weights\n",
    "        self.input_dim = input_dim\n",
    "        self.input_length = input_length\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_length, self.input_dim)\n",
    "        super(Convolution1D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[2]\n",
    "        self.W_shape = (self.filter_length, 1, input_dim, self.nb_filter)\n",
    "\n",
    "        self.W = self.add_weight(self.W_shape,\n",
    "                                 initializer=functools.partial(self.init,\n",
    "                                                               dim_ordering='th'),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((self.nb_filter,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        length = conv_output_length(input_shape[1],\n",
    "                                    self.filter_length,\n",
    "                                    self.border_mode,\n",
    "                                    self.subsample[0])\n",
    "        return (input_shape[0], length, self.nb_filter)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x = K.expand_dims(x, 2)  # add a dummy dimension\n",
    "        output = K.conv2d(x, self.W, strides=self.subsample,\n",
    "                          border_mode=self.border_mode,\n",
    "                          dim_ordering='tf')\n",
    "        output = K.squeeze(output, 2)  # remove the dummy dimension\n",
    "        if self.bias:\n",
    "            output += K.reshape(self.b, (1, 1, self.nb_filter))\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nb_filter': self.nb_filter,\n",
    "                  'filter_length': self.filter_length,\n",
    "                  'init': self.init.__name__,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'border_mode': self.border_mode,\n",
    "                  'subsample_length': self.subsample_length,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n",
    "                  'bias': self.bias,\n",
    "                  'input_dim': self.input_dim,\n",
    "                  'input_length': self.input_length}\n",
    "        base_config = super(Convolution1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class RevCompConv1D(Convolution1D):\n",
    "    '''Like Convolution1D, except the reverse-complement filters with tied\n",
    "    weights are added in the channel dimension. The reverse complement\n",
    "    of the channel at index i is at index -i.\n",
    "    # Example\n",
    "    ```python\n",
    "        # apply a reverse-complemented convolution 1d of length 20\n",
    "        # to a sequence with 100bp input, with 2*64 output filters\n",
    "        model = Sequential()\n",
    "        model.add(RevCompConv1D(nb_filter=64, filter_length=20,\n",
    "                                border_mode='same', input_shape=(100, 4)))\n",
    "        # now model.output_shape == (None, 100, 128)\n",
    "        # add a new reverse-complemented conv1d on top\n",
    "        model.add(RevCompConv1D(nb_filter=32, filter_length=10,\n",
    "                                border_mode='same'))\n",
    "        # now model.output_shape == (None, 10, 64)\n",
    "    ```\n",
    "    # Arguments\n",
    "        nb_filter: Number of non-reverse complemented convolution kernels\n",
    "            to use (half the dimensionality of the output).\n",
    "        filter_length: The extension (spatial or temporal) of each filter.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)),\n",
    "            or alternatively, Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights\n",
    "            (reverse-complemented portion should not be included as \n",
    "            it's applied during compilation)\n",
    "        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n",
    "        subsample_length: factor by which to subsample output.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        bias: whether to include a bias\n",
    "            (i.e. make the layer affine rather than linear).\n",
    "        input_dim: Number of channels/dimensions in the input.\n",
    "            Either this argument or the keyword argument `input_shape`must be\n",
    "            provided when using this layer as the first layer in a model.\n",
    "        input_length: Length of input sequences, when it is constant.\n",
    "            This argument is required if you are going to connect\n",
    "            `Flatten` then `Dense` layers upstream\n",
    "            (without it, the shape of the dense outputs cannot be computed).\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, input_dim)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, new_steps, nb_filter)`.\n",
    "        `steps` value might have changed due to padding.\n",
    "    '''\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        length = conv_output_length(input_shape[1],\n",
    "                                    self.filter_length,\n",
    "                                    self.border_mode,\n",
    "                                    self.subsample[0])\n",
    "        return (input_shape[0], length, 2*self.nb_filter)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #create a rev-comped W. The last axis is the output channel axis.\n",
    "        #dim 1 is dummy axis of size 1 (see 'build' method in Convolution1D)\n",
    "        #Rev comp is along both the length (dim 0) and input channel (dim 2)\n",
    "        #axes; that is the reason for ::-1, ::-1 in the first and third dims.\n",
    "        #The rev-comp of channel at index i should be at index -i\n",
    "        #This is the reason for the ::-1 in the last dim.\n",
    "        rev_comp_W = K.concatenate([self.W, self.W[::-1,:,::-1,::-1]],axis=-1)\n",
    "        if (self.bias):\n",
    "            rev_comp_b = K.concatenate([self.b, self.b[::-1]], axis=-1)\n",
    "        x = K.expand_dims(x, 2)  # add a dummy dimension\n",
    "        output = K.conv2d(x, rev_comp_W, strides=self.subsample,\n",
    "                          border_mode=self.border_mode,\n",
    "                          dim_ordering='tf')\n",
    "        output = K.squeeze(output, 2)  # remove the dummy dimension\n",
    "        if self.bias:\n",
    "            output += K.reshape(rev_comp_b, (1, 1, 2*self.nb_filter))\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class WeightedSum1D(Layer):\n",
    "    '''Learns a weight for each position, for each channel, and sums\n",
    "    lengthwise.\n",
    "    # Arguments\n",
    "        symmetric: if want weights to be symmetric along length, set to True\n",
    "        input_is_revcomp_conv: if the input is [RevCompConv1D], set to True for\n",
    "            added weight sharing between reverse-complement pairs\n",
    "        smoothness_penalty: penalty to be applied to absolute difference\n",
    "            of adjacent weights in the length dimension\n",
    "        bias: whether or not to have bias parameters\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)),\n",
    "            or alternatively, Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    '''\n",
    "    def __init__(self, symmetric, input_is_revcomp_conv,\n",
    "                       smoothness_penalty=None, bias=False,\n",
    "                       init='fanintimesfanouttimestwo', weights=None,\n",
    "                       **kwargs):\n",
    "        super(WeightedSum1D, self).__init__(**kwargs)\n",
    "        self.symmetric = symmetric\n",
    "        self.input_is_revcomp_conv = input_is_revcomp_conv\n",
    "        self.smoothness_penalty = smoothness_penalty\n",
    "        self.bias = bias\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        self.init = initializations.get(init)\n",
    "\n",
    "    def build(self, input_shape): \n",
    "        #input_shape[0] is the batch index\n",
    "        #input_shape[1] is length of input\n",
    "        #input_shape[2] is number of filters\n",
    "\n",
    "        if (self.symmetric == False):\n",
    "            W_length = input_shape[1]\n",
    "        else:\n",
    "            self.odd_input_length = input_shape[1]%2.0 == 1\n",
    "            #+0.5 below turns floor into ceil\n",
    "            W_length = int(input_shape[1]/2.0 + 0.5)\n",
    "\n",
    "        if (self.input_is_revcomp_conv == False):\n",
    "            W_chan = input_shape[2]\n",
    "        else:\n",
    "            assert input_shape[2]%2==0,\\\n",
    "             \"if input is revcomp conv, # incoming channels would be even\"\n",
    "            W_chan = int(input_shape[2]/2)\n",
    "\n",
    "        self.W_shape = (W_length, W_chan)\n",
    "        self.b_shape = (W_chan,)\n",
    "        self.W = self.add_weight(self.W_shape,\n",
    "             initializer=functools.partial(\n",
    "              self.init, dim_ordering='th'),\n",
    "             name='{}_W'.format(self.name),\n",
    "             regularizer=(None if self.smoothness_penalty is None else\n",
    "                         regularizers.SmoothnessRegularizer(\n",
    "                          self.smoothness_penalty)))\n",
    "        if (self.bias):\n",
    "            self.b = self.add_weight(self.b_shape,\n",
    "                 initializer=functools.partial(\n",
    "                  self.init, dim_ordering='th'),\n",
    "                 name='{}_b'.format(self.name))\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    #3D input -> 2D output (loses length dimension)\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if (self.symmetric == False):\n",
    "            W = self.W\n",
    "        else:\n",
    "            W = K.concatenate(\n",
    "                 tensors=[self.W,\n",
    "                          #reverse along length, concat along length\n",
    "                          self.W[::-1][(1 if self.odd_input_length else 0):]],\n",
    "                 axis=0)\n",
    "        if (self.bias):\n",
    "            b = self.b\n",
    "        if (self.input_is_revcomp_conv):\n",
    "            #reverse along both length and channel dims, concat along chan\n",
    "            #if symmetric=True, reversal along length here makes no diff\n",
    "            W = K.concatenate(tensors=[W, W[::-1,::-1]], axis=1)\n",
    "            if (self.bias):\n",
    "                b = K.concatenate(tensors=[b, b[::-1]], axis=0)\n",
    "        output = K.sum(x*K.expand_dims(W,0), axis=1)\n",
    "        if (self.bias):\n",
    "            output = output + K.expand_dims(b,0)\n",
    "        return output \n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'symmetric': self.symmetric,\n",
    "                  'input_is_revcomp_conv': self.input_is_revcomp_conv,\n",
    "                  'smoothness_penalty': self.smoothness_penalty,\n",
    "                  'bias': self.bias,\n",
    "                  'init': self.init.__name__}\n",
    "        base_config = super(WeightedSum1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class AtrousConvolution1D(Convolution1D):\n",
    "    '''Atrous Convolution operator for filtering neighborhoods of one-dimensional inputs.\n",
    "    A.k.a dilated convolution or convolution with holes.\n",
    "    When using this layer as the first layer in a model,\n",
    "    either provide the keyword argument `input_dim`\n",
    "    (int, e.g. 128 for sequences of 128-dimensional vectors),\n",
    "    or `input_shape` (tuples of integers, e.g. (10, 128) for sequences\n",
    "    of 10 vectors of 128-dimensional vectors).\n",
    "    # Example\n",
    "    ```python\n",
    "        # apply an atrous convolution 1d with atrous rate 2 of length 3 to a sequence with 10 timesteps,\n",
    "        # with 64 output filters\n",
    "        model = Sequential()\n",
    "        model.add(AtrousConvolution1D(64, 3, atrous_rate=2, border_mode='same', input_shape=(10, 32)))\n",
    "        # now model.output_shape == (None, 10, 64)\n",
    "        # add a new atrous conv1d on top\n",
    "        model.add(AtrousConvolution1D(32, 3, atrous_rate=2, border_mode='same'))\n",
    "        # now model.output_shape == (None, 10, 32)\n",
    "    ```\n",
    "    # Arguments\n",
    "        nb_filter: Number of convolution kernels to use\n",
    "            (dimensionality of the output).\n",
    "        filter_length: The extension (spatial or temporal) of each filter.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)),\n",
    "            or alternatively, Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n",
    "        subsample_length: factor by which to subsample output.\n",
    "        atrous_rate: Factor for kernel dilation. Also called filter_dilation\n",
    "            elsewhere.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        bias: whether to include a bias\n",
    "            (i.e. make the layer affine rather than linear).\n",
    "        input_dim: Number of channels/dimensions in the input.\n",
    "            Either this argument or the keyword argument `input_shape`must be\n",
    "            provided when using this layer as the first layer in a model.\n",
    "        input_length: Length of input sequences, when it is constant.\n",
    "            This argument is required if you are going to connect\n",
    "            `Flatten` then `Dense` layers upstream\n",
    "            (without it, the shape of the dense outputs cannot be computed).\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, input_dim)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, new_steps, nb_filter)`.\n",
    "        `steps` value might have changed due to padding.\n",
    "    '''\n",
    "    def __init__(self, nb_filter, filter_length,\n",
    "                 init='glorot_uniform', activation=None, weights=None,\n",
    "                 border_mode='valid', subsample_length=1, atrous_rate=1,\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        if border_mode not in {'valid', 'same', 'full'}:\n",
    "            raise ValueError('Invalid border mode for AtrousConv1D:', border_mode)\n",
    "\n",
    "        self.atrous_rate = int(atrous_rate)\n",
    "\n",
    "        super(AtrousConvolution1D, self).__init__(nb_filter, filter_length,\n",
    "                                                  init=init, activation=activation,\n",
    "                                                  weights=weights, border_mode=border_mode,\n",
    "                                                  subsample_length=subsample_length,\n",
    "                                                  W_regularizer=W_regularizer, b_regularizer=b_regularizer,\n",
    "                                                  activity_regularizer=activity_regularizer,\n",
    "                                                  W_constraint=W_constraint, b_constraint=b_constraint,\n",
    "                                                  bias=bias, **kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        length = conv_output_length(input_shape[1],\n",
    "                                    self.filter_length,\n",
    "                                    self.border_mode,\n",
    "                                    self.subsample[0],\n",
    "                                    dilation=self.atrous_rate)\n",
    "        return (input_shape[0], length, self.nb_filter)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x = K.expand_dims(x, 2)  # add a dummy dimension\n",
    "        output = K.conv2d(x, self.W, strides=self.subsample,\n",
    "                          border_mode=self.border_mode,\n",
    "                          dim_ordering='tf',\n",
    "                          filter_dilation=(self.atrous_rate, self.atrous_rate))\n",
    "        output = K.squeeze(output, 2)  # remove the dummy dimension\n",
    "        if self.bias:\n",
    "            output += K.reshape(self.b, (1, 1, self.nb_filter))\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'atrous_rate': self.atrous_rate}\n",
    "        base_config = super(AtrousConvolution1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class Convolution2D(Layer):\n",
    "    '''Convolution operator for filtering windows of two-dimensional inputs.\n",
    "    When using this layer as the first layer in a model,\n",
    "    provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does not include the sample axis),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    # Examples\n",
    "    ```python\n",
    "        # apply a 3x3 convolution with 64 output filters on a 256x256 image:\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(3, 256, 256)))\n",
    "        # now model.output_shape == (None, 64, 256, 256)\n",
    "        # add a 3x3 convolution on top, with 32 output filters:\n",
    "        model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "        # now model.output_shape == (None, 32, 256, 256)\n",
    "    ```\n",
    "    # Arguments\n",
    "        nb_filter: Number of convolution filters to use.\n",
    "        nb_row: Number of rows in the convolution kernel.\n",
    "        nb_col: Number of columns in the convolution kernel.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass\n",
    "            a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n",
    "        subsample: tuple of length 2. Factor by which to subsample output.\n",
    "            Also called strides elsewhere.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n",
    "            (the depth) is at index 1, in 'tf' mode is it at index 3.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "        bias: whether to include a bias\n",
    "            (i.e. make the layer affine rather than linear).\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, nb_filter, new_rows, new_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n",
    "        `rows` and `cols` values might have changed due to padding.\n",
    "    '''\n",
    "    def __init__(self, nb_filter, nb_row, nb_col,\n",
    "                 init='glorot_uniform', activation=None, weights=None,\n",
    "                 border_mode='valid', subsample=(1, 1), dim_ordering='default',\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        if border_mode not in {'valid', 'same', 'full'}:\n",
    "            raise ValueError('Invalid border mode for Convolution2D:', border_mode)\n",
    "        self.nb_filter = nb_filter\n",
    "        self.nb_row = nb_row\n",
    "        self.nb_col = nb_col\n",
    "        self.init = initializations.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.border_mode = border_mode\n",
    "        self.subsample = tuple(subsample)\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        self.initial_weights = weights\n",
    "        super(Convolution2D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            stack_size = input_shape[1]\n",
    "            self.W_shape = (self.nb_filter, stack_size, self.nb_row, self.nb_col)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            stack_size = input_shape[3]\n",
    "            self.W_shape = (self.nb_row, self.nb_col, stack_size, self.nb_filter)\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "        self.W = self.add_weight(self.W_shape,\n",
    "                                 initializer=functools.partial(self.init,\n",
    "                                                               dim_ordering=self.dim_ordering),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((self.nb_filter,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            rows = input_shape[2]\n",
    "            cols = input_shape[3]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "        rows = conv_output_length(rows, self.nb_row,\n",
    "                                  self.border_mode, self.subsample[0])\n",
    "        cols = conv_output_length(cols, self.nb_col,\n",
    "                                  self.border_mode, self.subsample[1])\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            return (input_shape[0], self.nb_filter, rows, cols)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            return (input_shape[0], rows, cols, self.nb_filter)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.conv2d(x, self.W, strides=self.subsample,\n",
    "                          border_mode=self.border_mode,\n",
    "                          dim_ordering=self.dim_ordering,\n",
    "                          filter_shape=self.W_shape)\n",
    "        if self.bias:\n",
    "            if self.dim_ordering == 'th':\n",
    "                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n",
    "            elif self.dim_ordering == 'tf':\n",
    "                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n",
    "            else:\n",
    "                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nb_filter': self.nb_filter,\n",
    "                  'nb_row': self.nb_row,\n",
    "                  'nb_col': self.nb_col,\n",
    "                  'init': self.init.__name__,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'border_mode': self.border_mode,\n",
    "                  'subsample': self.subsample,\n",
    "                  'dim_ordering': self.dim_ordering,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n",
    "                  'bias': self.bias}\n",
    "        base_config = super(Convolution2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class Deconvolution2D(Convolution2D):\n",
    "    '''Transposed convolution operator for filtering windows of two-dimensional inputs.\n",
    "    The need for transposed convolutions generally arises from the desire\n",
    "    to use a transformation going in the opposite direction of a normal convolution,\n",
    "    i.e., from something that has the shape of the output of some convolution\n",
    "    to something that has the shape of its input\n",
    "    while maintaining a connectivity pattern that is compatible with said convolution. [1]\n",
    "    When using this layer as the first layer in a model,\n",
    "    provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does not include the sample axis),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    To pass the correct `output_shape` to this layer,\n",
    "    one could use a test model to predict and observe the actual output shape.\n",
    "    # Examples\n",
    "    ```python\n",
    "        # apply a 3x3 transposed convolution with stride 1x1 and 3 output filters on a 12x12 image:\n",
    "        model = Sequential()\n",
    "        model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14), border_mode='valid', input_shape=(3, 12, 12)))\n",
    "        # Note that you will have to change the output_shape depending on the backend used.\n",
    "        # we can predict with the model and print the shape of the array.\n",
    "        dummy_input = np.ones((32, 3, 12, 12))\n",
    "        # For TensorFlow dummy_input = np.ones((32, 12, 12, 3))\n",
    "        preds = model.predict(dummy_input)\n",
    "        print(preds.shape)\n",
    "        # Theano GPU: (None, 3, 13, 13)\n",
    "        # Theano CPU: (None, 3, 14, 14)\n",
    "        # TensorFlow: (None, 14, 14, 3)\n",
    "        # apply a 3x3 transposed convolution with stride 2x2 and 3 output filters on a 12x12 image:\n",
    "        model = Sequential()\n",
    "        model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 25, 25), subsample=(2, 2), border_mode='valid', input_shape=(3, 12, 12)))\n",
    "        model.summary()\n",
    "        # we can predict with the model and print the shape of the array.\n",
    "        dummy_input = np.ones((32, 3, 12, 12))\n",
    "        # For TensorFlow dummy_input = np.ones((32, 12, 12, 3))\n",
    "        preds = model.predict(dummy_input)\n",
    "        print(preds.shape)\n",
    "        # Theano GPU: (None, 3, 25, 25)\n",
    "        # Theano CPU: (None, 3, 25, 25)\n",
    "        # TensorFlow: (None, 25, 25, 3)\n",
    "    ```\n",
    "    # Arguments\n",
    "        nb_filter: Number of transposed convolution filters to use.\n",
    "        nb_row: Number of rows in the transposed convolution kernel.\n",
    "        nb_col: Number of columns in the transposed convolution kernel.\n",
    "        output_shape: Output shape of the transposed convolution operation.\n",
    "            tuple of integers (nb_samples, nb_filter, nb_output_rows, nb_output_cols)\n",
    "            Formula for calculation of the output shape [1], [2]:\n",
    "                o = s (i - 1) + a + k - 2p, \\quad a \\in \\{0, \\ldots, s - 1\\}\n",
    "                where:\n",
    "                    i - input size (rows or cols),\n",
    "                    k - kernel size (nb_filter),\n",
    "                    s - stride (subsample for rows or cols respectively),\n",
    "                    p - padding size,\n",
    "                    a - user-specified quantity used to distinguish between\n",
    "                        the s different possible output sizes.\n",
    "             Because a is not specified explicitly and Theano and Tensorflow\n",
    "             use different values, it is better to use a dummy input and observe\n",
    "             the actual output shape of a layer as specified in the examples.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass\n",
    "            a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano/TensorFlow function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n",
    "        subsample: tuple of length 2. Factor by which to oversample output.\n",
    "            Also called strides elsewhere.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n",
    "            (the depth) is at index 1, in 'tf' mode is it at index 3.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, nb_filter, new_rows, new_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n",
    "        `rows` and `cols` values might have changed due to padding.\n",
    "    # References\n",
    "        [1] [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285 \"arXiv:1603.07285v1 [stat.ML]\")\n",
    "        [2] [Transposed convolution arithmetic](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic)\n",
    "        [3] [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n",
    "    '''\n",
    "    def __init__(self, nb_filter, nb_row, nb_col, output_shape,\n",
    "                 init='glorot_uniform', activation=None, weights=None,\n",
    "                 border_mode='valid', subsample=(1, 1),\n",
    "                 dim_ordering='default',\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        if border_mode not in {'valid', 'same', 'full'}:\n",
    "            raise ValueError('Invalid border mode for Deconvolution2D:', border_mode)\n",
    "\n",
    "        self.output_shape_ = output_shape\n",
    "\n",
    "        super(Deconvolution2D, self).__init__(nb_filter, nb_row, nb_col,\n",
    "                                              init=init,\n",
    "                                              activation=activation,\n",
    "                                              weights=weights,\n",
    "                                              border_mode=border_mode,\n",
    "                                              subsample=subsample,\n",
    "                                              dim_ordering=dim_ordering,\n",
    "                                              W_regularizer=W_regularizer,\n",
    "                                              b_regularizer=b_regularizer,\n",
    "                                              activity_regularizer=activity_regularizer,\n",
    "                                              W_constraint=W_constraint,\n",
    "                                              b_constraint=b_constraint,\n",
    "                                              bias=bias,\n",
    "                                              **kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            rows = self.output_shape_[2]\n",
    "            cols = self.output_shape_[3]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            rows = self.output_shape_[1]\n",
    "            cols = self.output_shape_[2]\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            return (input_shape[0], self.nb_filter, rows, cols)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            return (input_shape[0], rows, cols, self.nb_filter)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.deconv2d(x, self.W, self.output_shape_,\n",
    "                            strides=self.subsample,\n",
    "                            border_mode=self.border_mode,\n",
    "                            dim_ordering=self.dim_ordering,\n",
    "                            filter_shape=self.W_shape)\n",
    "        if self.bias:\n",
    "            if self.dim_ordering == 'th':\n",
    "                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n",
    "            elif self.dim_ordering == 'tf':\n",
    "                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n",
    "            else:\n",
    "                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_shape': self.output_shape_}\n",
    "        base_config = super(Deconvolution2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class AtrousConvolution2D(Convolution2D):\n",
    "    '''Atrous Convolution operator for filtering windows of two-dimensional inputs.\n",
    "    A.k.a dilated convolution or convolution with holes.\n",
    "    When using this layer as the first layer in a model,\n",
    "    provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does not include the sample axis),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    # Examples\n",
    "    ```python\n",
    "        # apply a 3x3 convolution with atrous rate 2x2 and 64 output filters on a 256x256 image:\n",
    "        model = Sequential()\n",
    "        model.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2), border_mode='valid', input_shape=(3, 256, 256)))\n",
    "        # now the actual kernel size is dilated from 3x3 to 5x5 (3+(3-1)*(2-1)=5)\n",
    "        # thus model.output_shape == (None, 64, 252, 252)\n",
    "    ```\n",
    "    # Arguments\n",
    "        nb_filter: Number of convolution filters to use.\n",
    "        nb_row: Number of rows in the convolution kernel.\n",
    "        nb_col: Number of columns in the convolution kernel.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass\n",
    "            a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n",
    "        subsample: tuple of length 2. Factor by which to subsample output.\n",
    "            Also called strides elsewhere.\n",
    "        atrous_rate: tuple of length 2. Factor for kernel dilation.\n",
    "            Also called filter_dilation elsewhere.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n",
    "            (the depth) is at index 1, in 'tf' mode is it at index 3.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, nb_filter, new_rows, new_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n",
    "        `rows` and `cols` values might have changed due to padding.\n",
    "    # References\n",
    "        - [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)\n",
    "    '''\n",
    "    def __init__(self, nb_filter, nb_row, nb_col,\n",
    "                 init='glorot_uniform', activation=None, weights=None,\n",
    "                 border_mode='valid', subsample=(1, 1),\n",
    "                 atrous_rate=(1, 1), dim_ordering='default',\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "\n",
    "        if border_mode not in {'valid', 'same', 'full'}:\n",
    "            raise ValueError('Invalid border mode for AtrousConv2D:', border_mode)\n",
    "\n",
    "        self.atrous_rate = tuple(atrous_rate)\n",
    "\n",
    "        super(AtrousConvolution2D, self).__init__(nb_filter, nb_row, nb_col,\n",
    "                                                  init=init,\n",
    "                                                  activation=activation,\n",
    "                                                  weights=weights,\n",
    "                                                  border_mode=border_mode,\n",
    "                                                  subsample=subsample,\n",
    "                                                  dim_ordering=dim_ordering,\n",
    "                                                  W_regularizer=W_regularizer,\n",
    "                                                  b_regularizer=b_regularizer,\n",
    "                                                  activity_regularizer=activity_regularizer,\n",
    "                                                  W_constraint=W_constraint,\n",
    "                                                  b_constraint=b_constraint,\n",
    "                                                  bias=bias,\n",
    "                                                  **kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            rows = input_shape[2]\n",
    "            cols = input_shape[3]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "        rows = conv_output_length(rows, self.nb_row, self.border_mode,\n",
    "                                  self.subsample[0],\n",
    "                                  dilation=self.atrous_rate[0])\n",
    "        cols = conv_output_length(cols, self.nb_col, self.border_mode,\n",
    "                                  self.subsample[1],\n",
    "                                  dilation=self.atrous_rate[1])\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            return (input_shape[0], self.nb_filter, rows, cols)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            return (input_shape[0], rows, cols, self.nb_filter)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.conv2d(x, self.W, strides=self.subsample,\n",
    "                          border_mode=self.border_mode,\n",
    "                          dim_ordering=self.dim_ordering,\n",
    "                          filter_shape=self.W_shape,\n",
    "                          filter_dilation=self.atrous_rate)\n",
    "        if self.bias:\n",
    "            if self.dim_ordering == 'th':\n",
    "                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n",
    "            elif self.dim_ordering == 'tf':\n",
    "                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n",
    "            else:\n",
    "                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'atrous_rate': self.atrous_rate}\n",
    "        base_config = super(AtrousConvolution2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class SeparableConvolution2D(Layer):\n",
    "    '''Separable convolution operator for 2D inputs.\n",
    "    Separable convolutions consist in first performing\n",
    "    a depthwise spatial convolution\n",
    "    (which acts on each input channel separately)\n",
    "    followed by a pointwise convolution which mixes together the resulting\n",
    "    output channels. The `depth_multiplier` argument controls how many\n",
    "    output channels are generated per input channel in the depthwise step.\n",
    "    Intuitively, separable convolutions can be understood as\n",
    "    a way to factorize a convolution kernel into two smaller kernels,\n",
    "    or as an extreme version of an Inception block.\n",
    "    When using this layer as the first layer in a model,\n",
    "    provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does not include the sample axis),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    # Theano warning\n",
    "    This layer is only available with the\n",
    "    TensorFlow backend for the time being.\n",
    "    # Arguments\n",
    "        nb_filter: Number of convolution filters to use.\n",
    "        nb_row: Number of rows in the convolution kernel.\n",
    "        nb_col: Number of columns in the convolution kernel.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass\n",
    "            a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "        border_mode: 'valid' or 'same'.\n",
    "        subsample: tuple of length 2. Factor by which to subsample output.\n",
    "            Also called strides elsewhere.\n",
    "        depth_multiplier: how many output channel to use per input channel\n",
    "            for the depthwise convolution step.\n",
    "        depthwise_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the depthwise weights matrix.\n",
    "        pointwise_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the pointwise weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        depthwise_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the depthwise weights matrix.\n",
    "        pointwise_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the pointwise weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n",
    "            (the depth) is at index 1, in 'tf' mode is it at index 3.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "        bias: whether to include a bias\n",
    "            (i.e. make the layer affine rather than linear).\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, nb_filter, new_rows, new_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n",
    "        `rows` and `cols` values might have changed due to padding.\n",
    "    '''\n",
    "    def __init__(self, nb_filter, nb_row, nb_col,\n",
    "                 init='glorot_uniform', activation=None, weights=None,\n",
    "                 border_mode='valid', subsample=(1, 1),\n",
    "                 depth_multiplier=1, dim_ordering='default',\n",
    "                 depthwise_regularizer=None, pointwise_regularizer=None,\n",
    "                 b_regularizer=None, activity_regularizer=None,\n",
    "                 depthwise_constraint=None, pointwise_constraint=None,\n",
    "                 b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        if K.backend() != 'tensorflow':\n",
    "            raise RuntimeError('SeparableConv2D is only available '\n",
    "                               'with TensorFlow for the time being.')\n",
    "\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "\n",
    "        if border_mode not in {'valid', 'same'}:\n",
    "            raise ValueError('Invalid border mode for SeparableConv2D:', border_mode)\n",
    "\n",
    "        if border_mode not in {'valid', 'same'}:\n",
    "            raise ValueError('Invalid border mode for SeparableConv2D:', border_mode)\n",
    "        self.nb_filter = nb_filter\n",
    "        self.nb_row = nb_row\n",
    "        self.nb_col = nb_col\n",
    "        self.init = initializations.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        if border_mode not in {'valid', 'same'}:\n",
    "            raise ValueError('border_mode must be in {valid, same}.')\n",
    "        self.border_mode = border_mode\n",
    "        self.subsample = tuple(subsample)\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "\n",
    "        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n",
    "        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.depthwise_constraint = constraints.get(depthwise_constraint)\n",
    "        self.pointwise_constraint = constraints.get(pointwise_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        self.initial_weights = weights\n",
    "        super(SeparableConvolution2D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            stack_size = input_shape[1]\n",
    "            depthwise_shape = (self.depth_multiplier, stack_size, self.nb_row, self.nb_col)\n",
    "            pointwise_shape = (self.nb_filter, self.depth_multiplier * stack_size, 1, 1)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            stack_size = input_shape[3]\n",
    "            depthwise_shape = (self.nb_row, self.nb_col, stack_size, self.depth_multiplier)\n",
    "            pointwise_shape = (1, 1, self.depth_multiplier * stack_size, self.nb_filter)\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "        self.depthwise_kernel = self.add_weight(depthwise_shape,\n",
    "                                                initializer=functools.partial(self.init,\n",
    "                                                                              dim_ordering=self.dim_ordering),\n",
    "                                                regularizer=self.depthwise_regularizer,\n",
    "                                                constraint=self.depthwise_constraint,\n",
    "                                                name='{}_depthwise_kernel'.format(self.name))\n",
    "        self.pointwise_kernel = self.add_weight(pointwise_shape,\n",
    "                                                initializer=functools.partial(self.init,\n",
    "                                                                              dim_ordering=self.dim_ordering),\n",
    "                                                regularizer=self.pointwise_regularizer,\n",
    "                                                constraint=self.pointwise_constraint,\n",
    "                                                name='{}_pointwise_kernel'.format(self.name))\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((self.nb_filter,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            rows = input_shape[2]\n",
    "            cols = input_shape[3]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "        rows = conv_output_length(rows, self.nb_row,\n",
    "                                  self.border_mode, self.subsample[0])\n",
    "        cols = conv_output_length(cols, self.nb_col,\n",
    "                                  self.border_mode, self.subsample[1])\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            return (input_shape[0], self.nb_filter, rows, cols)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            return (input_shape[0], rows, cols, self.nb_filter)\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.separable_conv2d(x, self.depthwise_kernel,\n",
    "                                    self.pointwise_kernel,\n",
    "                                    strides=self.subsample,\n",
    "                                    border_mode=self.border_mode,\n",
    "                                    dim_ordering=self.dim_ordering)\n",
    "        if self.bias:\n",
    "            if self.dim_ordering == 'th':\n",
    "                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n",
    "            elif self.dim_ordering == 'tf':\n",
    "                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n",
    "            else:\n",
    "                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nb_filter': self.nb_filter,\n",
    "                  'nb_row': self.nb_row,\n",
    "                  'nb_col': self.nb_col,\n",
    "                  'init': self.init.__name__,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'border_mode': self.border_mode,\n",
    "                  'subsample': self.subsample,\n",
    "                  'depth_multiplier': self.depth_multiplier,\n",
    "                  'dim_ordering': self.dim_ordering,\n",
    "                  'depthwise_regularizer': self.depthwise_regularizer.get_config() if self.depthwise_regularizer else None,\n",
    "                  'pointwise_regularizer': self.depthwise_regularizer.get_config() if self.depthwise_regularizer else None,\n",
    "                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'depthwise_constraint': self.depthwise_constraint.get_config() if self.depthwise_constraint else None,\n",
    "                  'pointwise_constraint': self.pointwise_constraint.get_config() if self.pointwise_constraint else None,\n",
    "                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n",
    "                  'bias': self.bias}\n",
    "        base_config = super(SeparableConvolution2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class Convolution3D(Layer):\n",
    "    '''Convolution operator for filtering windows of three-dimensional inputs.\n",
    "    When using this layer as the first layer in a model,\n",
    "    provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does not include the sample axis),\n",
    "    e.g. `input_shape=(3, 10, 128, 128)` for 10 frames of 128x128 RGB pictures.\n",
    "    # Arguments\n",
    "        nb_filter: Number of convolution filters to use.\n",
    "        kernel_dim1: Length of the first dimension in the convolution kernel.\n",
    "        kernel_dim2: Length of the second dimension in the convolution kernel.\n",
    "        kernel_dim3: Length of the third dimension in the convolution kernel.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass\n",
    "            a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of Numpy arrays to set as initial weights.\n",
    "        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n",
    "        subsample: tuple of length 3. Factor by which to subsample output.\n",
    "            Also called strides elsewhere.\n",
    "            Note: 'subsample' is implemented by slicing the output of conv3d with strides=(1,1,1).\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        b_constraint: instance of the [constraints](../constraints.md) module,\n",
    "            applied to the bias.\n",
    "        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n",
    "            (the depth) is at index 1, in 'tf' mode is it at index 4.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n",
    "    # Input shape\n",
    "        5D tensor with shape:\n",
    "        `(samples, channels, conv_dim1, conv_dim2, conv_dim3)` if dim_ordering='th'\n",
    "        or 5D tensor with shape:\n",
    "        `(samples, conv_dim1, conv_dim2, conv_dim3, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        5D tensor with shape:\n",
    "        `(samples, nb_filter, new_conv_dim1, new_conv_dim2, new_conv_dim3)` if dim_ordering='th'\n",
    "        or 5D tensor with shape:\n",
    "        `(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, nb_filter)` if dim_ordering='tf'.\n",
    "        `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have changed due to padding.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nb_filter, kernel_dim1, kernel_dim2, kernel_dim3,\n",
    "                 init='glorot_uniform', activation=None, weights=None,\n",
    "                 border_mode='valid', subsample=(1, 1, 1), dim_ordering='default',\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "\n",
    "        if border_mode not in {'valid', 'same', 'full'}:\n",
    "            raise ValueError('Invalid border mode for Convolution3D:', border_mode)\n",
    "        self.nb_filter = nb_filter\n",
    "        self.kernel_dim1 = kernel_dim1\n",
    "        self.kernel_dim2 = kernel_dim2\n",
    "        self.kernel_dim3 = kernel_dim3\n",
    "        self.init = initializations.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.border_mode = border_mode\n",
    "        self.subsample = tuple(subsample)\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.input_spec = [InputSpec(ndim=5)]\n",
    "        self.initial_weights = weights\n",
    "        super(Convolution3D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 5\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            stack_size = input_shape[1]\n",
    "            self.W_shape = (self.nb_filter, stack_size,\n",
    "                            self.kernel_dim1, self.kernel_dim2, self.kernel_dim3)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            stack_size = input_shape[4]\n",
    "            self.W_shape = (self.kernel_dim1, self.kernel_dim2, self.kernel_dim3,\n",
    "                            stack_size, self.nb_filter)\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "        self.W = self.add_weight(self.W_shape,\n",
    "                                 initializer=functools.partial(self.init,\n",
    "                                                               dim_ordering=self.dim_ordering),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((self.nb_filter,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            conv_dim1 = input_shape[2]\n",
    "            conv_dim2 = input_shape[3]\n",
    "            conv_dim3 = input_shape[4]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            conv_dim1 = input_shape[1]\n",
    "            conv_dim2 = input_shape[2]\n",
    "            conv_dim3 = input_shape[3]\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "        conv_dim1 = conv_output_length(conv_dim1, self.kernel_dim1,\n",
    "                                       self.border_mode, self.subsample[0])\n",
    "        conv_dim2 = conv_output_length(conv_dim2, self.kernel_dim2,\n",
    "                                       self.border_mode, self.subsample[1])\n",
    "        conv_dim3 = conv_output_length(conv_dim3, self.kernel_dim3,\n",
    "                                       self.border_mode, self.subsample[2])\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            return (input_shape[0], self.nb_filter, conv_dim1, conv_dim2, conv_dim3)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            return (input_shape[0], conv_dim1, conv_dim2, conv_dim3, self.nb_filter)\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        output = K.conv3d(x, self.W, strides=self.subsample,\n",
    "                          border_mode=self.border_mode,\n",
    "                          dim_ordering=self.dim_ordering,\n",
    "                          volume_shape=input_shape,\n",
    "                          filter_shape=self.W_shape)\n",
    "        if self.bias:\n",
    "            if self.dim_ordering == 'th':\n",
    "                output += K.reshape(self.b, (1, self.nb_filter, 1, 1, 1))\n",
    "            elif self.dim_ordering == 'tf':\n",
    "                output += K.reshape(self.b, (1, 1, 1, 1, self.nb_filter))\n",
    "            else:\n",
    "                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'nb_filter': self.nb_filter,\n",
    "                  'kernel_dim1': self.kernel_dim1,\n",
    "                  'kernel_dim2': self.kernel_dim2,\n",
    "                  'kernel_dim3': self.kernel_dim3,\n",
    "                  'dim_ordering': self.dim_ordering,\n",
    "                  'init': self.init.__name__,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'border_mode': self.border_mode,\n",
    "                  'subsample': self.subsample,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n",
    "                  'bias': self.bias}\n",
    "        base_config = super(Convolution3D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class UpSampling1D(Layer):\n",
    "    '''Repeat each temporal step `length` times along the time axis.\n",
    "    # Arguments\n",
    "        length: integer. Upsampling factor.\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, upsampled_steps, features)`.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, length=2, **kwargs):\n",
    "        self.length = length\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(UpSampling1D, self).__init__(**kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        length = self.length * input_shape[1] if input_shape[1] is not None else None\n",
    "        return (input_shape[0], length, input_shape[2])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.repeat_elements(x, self.length, axis=1)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'length': self.length}\n",
    "        base_config = super(UpSampling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class UpSampling2D(Layer):\n",
    "    '''Repeat the rows and columns of the data\n",
    "    by size[0] and size[1] respectively.\n",
    "    # Arguments\n",
    "        size: tuple of 2 integers. The upsampling factors for rows and columns.\n",
    "        dim_ordering: 'th' or 'tf'.\n",
    "            In 'th' mode, the channels dimension (the depth)\n",
    "            is at index 1, in 'tf' mode is it at index 3.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, upsampled_rows, upsampled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, upsampled_rows, upsampled_cols, channels)` if dim_ordering='tf'.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size=(2, 2), dim_ordering='default', **kwargs):\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        self.size = tuple(size)\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(UpSampling2D, self).__init__(**kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            width = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n",
    "            height = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    width,\n",
    "                    height)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            width = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
    "            height = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    width,\n",
    "                    height,\n",
    "                    input_shape[3])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.resize_images(x, self.size[0], self.size[1],\n",
    "                               self.dim_ordering)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'size': self.size}\n",
    "        base_config = super(UpSampling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class UpSampling3D(Layer):\n",
    "    '''Repeat the first, second and third dimension of the data\n",
    "    by size[0], size[1] and size[2] respectively.\n",
    "    # Arguments\n",
    "        size: tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.\n",
    "        dim_ordering: 'th' or 'tf'.\n",
    "            In 'th' mode, the channels dimension (the depth)\n",
    "            is at index 1, in 'tf' mode is it at index 4.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "    # Input shape\n",
    "        5D tensor with shape:\n",
    "        `(samples, channels, dim1, dim2, dim3)` if dim_ordering='th'\n",
    "        or 5D tensor with shape:\n",
    "        `(samples, dim1, dim2, dim3, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        5D tensor with shape:\n",
    "        `(samples, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)` if dim_ordering='th'\n",
    "        or 5D tensor with shape:\n",
    "        `(samples, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)` if dim_ordering='tf'.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size=(2, 2, 2), dim_ordering='default', **kwargs):\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        self.size = tuple(size)\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=5)]\n",
    "        super(UpSampling3D, self).__init__(**kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            dim1 = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n",
    "            dim2 = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n",
    "            dim3 = self.size[2] * input_shape[4] if input_shape[4] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    dim1,\n",
    "                    dim2,\n",
    "                    dim3)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            dim1 = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
    "            dim2 = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
    "            dim3 = self.size[2] * input_shape[3] if input_shape[3] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    dim1,\n",
    "                    dim2,\n",
    "                    dim3,\n",
    "                    input_shape[4])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.resize_volumes(x, self.size[0], self.size[1], self.size[2],\n",
    "                                self.dim_ordering)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'size': self.size}\n",
    "        base_config = super(UpSampling3D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class ZeroPadding1D(Layer):\n",
    "    '''Zero-padding layer for 1D input (e.g. temporal sequence).\n",
    "    # Arguments\n",
    "        padding: int, or tuple of int (length 2), or dictionary.\n",
    "            - If int:\n",
    "            How many zeros to add at the beginning and end of\n",
    "            the padding dimension (axis 1).\n",
    "            - If tuple of int (length 2)\n",
    "            How many zeros to add at the beginning and at the end of\n",
    "            the padding dimension, in order '(left_pad, right_pad)'.\n",
    "            - If dictionary: should contain the keys\n",
    "            {'left_pad', 'right_pad'}.\n",
    "            If any key is missing, default value of 0 will be used for the missing key.\n",
    "    # Input shape\n",
    "        3D tensor with shape (samples, axis_to_pad, features)\n",
    "    # Output shape\n",
    "        3D tensor with shape (samples, padded_axis, features)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, padding=1, **kwargs):\n",
    "        super(ZeroPadding1D, self).__init__(**kwargs)\n",
    "        self.padding = padding\n",
    "\n",
    "        if isinstance(padding, int):\n",
    "            self.left_pad = padding\n",
    "            self.right_pad = padding\n",
    "\n",
    "        elif isinstance(padding, dict):\n",
    "            if set(padding.keys()) <= {'left_pad', 'right_pad'}:\n",
    "                self.left_pad = padding.get('left_pad', 0)\n",
    "                self.right_pad = padding.get('right_pad', 0)\n",
    "            else:\n",
    "                raise ValueError('Unexpected key found in `padding` dictionary. '\n",
    "                                 'Keys have to be in {\"left_pad\", \"right_pad\"}. '\n",
    "                                 'Found: ' + str(padding.keys()))\n",
    "        else:\n",
    "            padding = tuple(padding)\n",
    "            if len(padding) != 2:\n",
    "                raise ValueError('`padding` should be int, or dict with keys '\n",
    "                                 '{\"left_pad\", \"right_pad\"}, or tuple of length 2. '\n",
    "                                 'Found: ' + str(padding))\n",
    "            self.left_pad = padding[0]\n",
    "            self.right_pad = padding[1]\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        length = input_shape[1] + self.left_pad + self.right_pad if input_shape[1] is not None else None\n",
    "        return (input_shape[0],\n",
    "                length,\n",
    "                input_shape[2])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.asymmetric_temporal_padding(x, left_pad=self.left_pad, right_pad=self.right_pad)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding}\n",
    "        base_config = super(ZeroPadding1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class ZeroPadding2D(Layer):\n",
    "    '''Zero-padding layer for 2D input (e.g. picture).\n",
    "    # Arguments\n",
    "        padding: tuple of int (length 2), or tuple of int (length 4), or dictionary.\n",
    "            - If tuple of int (length 2):\n",
    "            How many zeros to add at the beginning and end of\n",
    "            the 2 padding dimensions (rows and cols).\n",
    "            - If tuple of int (length 4):\n",
    "            How many zeros to add at the beginning and at the end of\n",
    "            the 2 padding dimensions (rows and cols), in the order\n",
    "            '(top_pad, bottom_pad, left_pad, right_pad)'.\n",
    "            - If dictionary: should contain the keys\n",
    "            {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}.\n",
    "            If any key is missing, default value of 0 will be used for the missing key.\n",
    "        dim_ordering: 'th' or 'tf'.\n",
    "            In 'th' mode, the channels dimension (the depth)\n",
    "            is at index 1, in 'tf' mode is it at index 3.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, padded_rows, padded_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, padded_rows, padded_cols, channels)` if dim_ordering='tf'.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 padding=(1, 1),\n",
    "                 dim_ordering='default',\n",
    "                 **kwargs):\n",
    "        super(ZeroPadding2D, self).__init__(**kwargs)\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "\n",
    "        self.padding = padding\n",
    "        if isinstance(padding, dict):\n",
    "            if set(padding.keys()) <= {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}:\n",
    "                self.top_pad = padding.get('top_pad', 0)\n",
    "                self.bottom_pad = padding.get('bottom_pad', 0)\n",
    "                self.left_pad = padding.get('left_pad', 0)\n",
    "                self.right_pad = padding.get('right_pad', 0)\n",
    "            else:\n",
    "                raise ValueError('Unexpected key found in `padding` dictionary. '\n",
    "                                 'Keys have to be in {\"top_pad\", \"bottom_pad\", '\n",
    "                                 '\"left_pad\", \"right_pad\"}.'\n",
    "                                 'Found: ' + str(padding.keys()))\n",
    "        else:\n",
    "            padding = tuple(padding)\n",
    "            if len(padding) == 2:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[0]\n",
    "                self.left_pad = padding[1]\n",
    "                self.right_pad = padding[1]\n",
    "            elif len(padding) == 4:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[1]\n",
    "                self.left_pad = padding[2]\n",
    "                self.right_pad = padding[3]\n",
    "            else:\n",
    "                raise TypeError('`padding` should be tuple of int '\n",
    "                                'of length 2 or 4, or dict. '\n",
    "                                'Found: ' + str(padding))\n",
    "\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            rows = input_shape[2] + self.top_pad + self.bottom_pad if input_shape[2] is not None else None\n",
    "            cols = input_shape[3] + self.left_pad + self.right_pad if input_shape[3] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    rows,\n",
    "                    cols)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            rows = input_shape[1] + self.top_pad + self.bottom_pad if input_shape[1] is not None else None\n",
    "            cols = input_shape[2] + self.left_pad + self.right_pad if input_shape[2] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    input_shape[3])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.asymmetric_spatial_2d_padding(x,\n",
    "                                               top_pad=self.top_pad,\n",
    "                                               bottom_pad=self.bottom_pad,\n",
    "                                               left_pad=self.left_pad,\n",
    "                                               right_pad=self.right_pad,\n",
    "                                               dim_ordering=self.dim_ordering)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding}\n",
    "        base_config = super(ZeroPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class ZeroPadding3D(Layer):\n",
    "    '''Zero-padding layer for 3D data (spatial or spatio-temporal).\n",
    "    # Arguments\n",
    "        padding: tuple of int (length 3)\n",
    "            How many zeros to add at the beginning and end of\n",
    "            the 3 padding dimensions (axis 3, 4 and 5).\n",
    "            Currently only symmetric padding is supported.\n",
    "        dim_ordering: 'th' or 'tf'.\n",
    "            In 'th' mode, the channels dimension (the depth)\n",
    "            is at index 1, in 'tf' mode is it at index 4.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "    # Input shape\n",
    "        5D tensor with shape:\n",
    "        (samples, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)\n",
    "    # Output shape\n",
    "        5D tensor with shape:\n",
    "        (samples, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, padding=(1, 1, 1), dim_ordering='default', **kwargs):\n",
    "        super(ZeroPadding3D, self).__init__(**kwargs)\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        self.padding = tuple(padding)\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=5)]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            dim1 = input_shape[2] + 2 * self.padding[0] if input_shape[2] is not None else None\n",
    "            dim2 = input_shape[3] + 2 * self.padding[1] if input_shape[3] is not None else None\n",
    "            dim3 = input_shape[4] + 2 * self.padding[2] if input_shape[4] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    dim1,\n",
    "                    dim2,\n",
    "                    dim3)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            dim1 = input_shape[1] + 2 * self.padding[0] if input_shape[1] is not None else None\n",
    "            dim2 = input_shape[2] + 2 * self.padding[1] if input_shape[2] is not None else None\n",
    "            dim3 = input_shape[3] + 2 * self.padding[2] if input_shape[3] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    dim1,\n",
    "                    dim2,\n",
    "                    dim3,\n",
    "                    input_shape[4])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.spatial_3d_padding(x, padding=self.padding,\n",
    "                                    dim_ordering=self.dim_ordering)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding}\n",
    "        base_config = super(ZeroPadding3D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class Cropping1D(Layer):\n",
    "    '''Cropping layer for 1D input (e.g. temporal sequence).\n",
    "    It crops along the time dimension (axis 1).\n",
    "    # Arguments\n",
    "        cropping: tuple of int (length 2)\n",
    "            How many units should be trimmed off at the beginning and end of\n",
    "            the cropping dimension (axis 1).\n",
    "    # Input shape\n",
    "        3D tensor with shape (samples, axis_to_crop, features)\n",
    "    # Output shape\n",
    "        3D tensor with shape (samples, cropped_axis, features)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, cropping=(1, 1), **kwargs):\n",
    "        super(Cropping1D, self).__init__(**kwargs)\n",
    "        self.cropping = tuple(cropping)\n",
    "        if len(self.cropping) != 2:\n",
    "            raise ValueError('`cropping` must be a tuple length of 2.')\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.built = True\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if input_shape[1] is not None:\n",
    "            length = input_shape[1] - self.cropping[0] - self.cropping[1]\n",
    "        else:\n",
    "            length = None\n",
    "        return (input_shape[0],\n",
    "                length,\n",
    "                input_shape[2])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.cropping[1] == 0:\n",
    "            return x[:, self.cropping[0]:, :]\n",
    "        else:\n",
    "            return x[:, self.cropping[0]:-self.cropping[1], :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'cropping': self.cropping}\n",
    "        base_config = super(Cropping1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class Cropping2D(Layer):\n",
    "    '''Cropping layer for 2D input (e.g. picture).\n",
    "    It crops along spatial dimensions, i.e. width and height.\n",
    "    # Arguments\n",
    "        cropping: tuple of tuple of int (length 2)\n",
    "            How many units should be trimmed off at the beginning and end of\n",
    "            the 2 cropping dimensions (width, height).\n",
    "        dim_ordering: 'th' or 'tf'.\n",
    "            In 'th' mode, the channels dimension (the depth)\n",
    "            is at index 1, in 'tf' mode is it at index 3.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        (samples, depth, first_axis_to_crop, second_axis_to_crop)\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        (samples, depth, first_cropped_axis, second_cropped_axis)\n",
    "    # Examples\n",
    "    ```python\n",
    "        # Crop the input 2D images or feature maps\n",
    "        model = Sequential()\n",
    "        model.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(3, 28, 28)))\n",
    "        # now model.output_shape == (None, 3, 24, 20)\n",
    "        model.add(Convolution2D(64, 3, 3, border_mode='same))\n",
    "        model.add(Cropping2D(cropping=((2, 2), (2, 2))))\n",
    "        # now model.output_shape == (None, 64, 20, 16)\n",
    "    ```\n",
    "    '''\n",
    "\n",
    "    def __init__(self, cropping=((0, 0), (0, 0)), dim_ordering='default', **kwargs):\n",
    "        super(Cropping2D, self).__init__(**kwargs)\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        self.cropping = tuple(cropping)\n",
    "        if len(self.cropping) != 2:\n",
    "            raise ValueError('`cropping` must be a tuple length of 2.')\n",
    "        if len(self.cropping[0]) != 2:\n",
    "            raise ValueError('`cropping[0]` must be a tuple length of 2.')\n",
    "        if len(self.cropping[1]) != 2:\n",
    "            raise ValueError('`cropping[1]` must be a tuple length of 2.')\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.built = True\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    input_shape[2] - self.cropping[0][0] - self.cropping[0][1],\n",
    "                    input_shape[3] - self.cropping[1][0] - self.cropping[1][1])\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1] - self.cropping[0][0] - self.cropping[0][1],\n",
    "                    input_shape[2] - self.cropping[1][0] - self.cropping[1][1],\n",
    "                    input_shape[3])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.dim_ordering == 'th':\n",
    "            return x[:,\n",
    "                     :,\n",
    "                     self.cropping[0][0]:-self.cropping[0][1],\n",
    "                     self.cropping[1][0]:-self.cropping[1][1]]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            return x[:,\n",
    "                     self.cropping[0][0]:-self.cropping[0][1],\n",
    "                     self.cropping[1][0]:-self.cropping[1][1],\n",
    "                     :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'cropping': self.cropping}\n",
    "        base_config = super(Cropping2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class Cropping3D(Layer):\n",
    "    '''Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n",
    "    # Arguments\n",
    "        cropping: tuple of tuple of int (length 3)\n",
    "            How many units should be trimmed off at the beginning and end of\n",
    "            the 3 cropping dimensions (kernel_dim1, kernel_dim2, kernerl_dim3).\n",
    "        dim_ordering: 'th' or 'tf'.\n",
    "            In 'th' mode, the channels dimension (the depth)\n",
    "            is at index 1, in 'tf' mode is it at index 4.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"tf\".\n",
    "    # Input shape\n",
    "        5D tensor with shape:\n",
    "        (samples, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)\n",
    "    # Output shape\n",
    "        5D tensor with shape:\n",
    "        (samples, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, cropping=((1, 1), (1, 1), (1, 1)),\n",
    "                 dim_ordering='default', **kwargs):\n",
    "        super(Cropping3D, self).__init__(**kwargs)\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        self.cropping = tuple(cropping)\n",
    "        if len(self.cropping) != 3:\n",
    "            raise ValueError('`cropping` must be a tuple length of 3.')\n",
    "        if len(self.cropping[0]) != 2:\n",
    "            raise ValueError('`cropping[0]` must be a tuple length of 2.')\n",
    "        if len(self.cropping[1]) != 2:\n",
    "            raise ValueError('`cropping[1]` must be a tuple length of 2.')\n",
    "        if len(self.cropping[2]) != 2:\n",
    "            raise ValueError('`cropping[2]` must be a tuple length of 2.')\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise ValueError('dim_ordering must be in {tf, th}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=5)]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.built = True\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            dim1 = input_shape[2] - self.cropping[0][0] - self.cropping[0][1] if input_shape[2] is not None else None\n",
    "            dim2 = input_shape[3] - self.cropping[1][0] - self.cropping[1][1] if input_shape[3] is not None else None\n",
    "            dim3 = input_shape[4] - self.cropping[2][0] - self.cropping[2][1] if input_shape[4] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    dim1,\n",
    "                    dim2,\n",
    "                    dim3)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            dim1 = input_shape[1] - self.cropping[0][0] - self.cropping[0][1] if input_shape[1] is not None else None\n",
    "            dim2 = input_shape[2] - self.cropping[1][0] - self.cropping[1][1] if input_shape[2] is not None else None\n",
    "            dim3 = input_shape[3] - self.cropping[2][0] - self.cropping[2][1] if input_shape[3] is not None else None\n",
    "            return (input_shape[0],\n",
    "                    dim1,\n",
    "                    dim2,\n",
    "                    dim3,\n",
    "                    input_shape[4])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.dim_ordering == 'th':\n",
    "            return x[:,\n",
    "                     :,\n",
    "                     self.cropping[0][0]:-self.cropping[0][1],\n",
    "                     self.cropping[1][0]:-self.cropping[1][1],\n",
    "                     self.cropping[2][0]:-self.cropping[2][1]]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            return x[:,\n",
    "                     self.cropping[0][0]:-self.cropping[0][1],\n",
    "                     self.cropping[1][0]:-self.cropping[1][1],\n",
    "                     self.cropping[2][0]:-self.cropping[2][1],\n",
    "                     :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'cropping': self.cropping}\n",
    "        base_config = super(Cropping3D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "# Aliases\n",
    "\n",
    "Conv1D = Convolution1D\n",
    "Conv2D = Convolution2D\n",
    "Conv3D = Convolution3D\n",
    "Deconv2D = Deconvolution2D\n",
    "AtrousConv1D = AtrousConvolution1D\n",
    "AtrousConv2D = AtrousConvolution2D\n",
    "SeparableConv2D = SeparableConvolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#install keras from https://github.com/kundajelab/keras/tree/keras_1\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "#build a sample model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.convolutional.RevCompConv1D(input_shape=(100,4),\n",
    "                                                   nb_filter=10,\n",
    "                                                   filter_length=11))\n",
    "model.add(keras.layers.normalization.RevCompConv1DBatchNorm())\n",
    "model.add(keras.layers.core.Activation(\"relu\"))\n",
    "model.add(keras.layers.convolutional.RevCompConv1D(nb_filter=10,\n",
    "                                                   filter_length=11))\n",
    "model.add(keras.layers.normalization.RevCompConv1DBatchNorm())\n",
    "model.add(keras.layers.core.Activation(\"relu\"))\n",
    "model.add(keras.layers.convolutional.RevCompConv1D(nb_filter=10,\n",
    "                                                   filter_length=11))\n",
    "model.add(keras.layers.normalization.RevCompConv1DBatchNorm())\n",
    "model.add(keras.layers.core.Activation(\"relu\"))\n",
    "model.add(keras.layers.pooling.MaxPooling1D(pool_length=10))\n",
    "model.add(keras.layers.convolutional.WeightedSum1D(symmetric=False,\n",
    "                                                   input_is_revcomp_conv=True,\n",
    "                                                   bias=False,\n",
    "                                                   init=\"fanintimesfanouttimestwo\"))\n",
    "model.add(keras.layers.core.DenseAfterRevcompWeightedSum(output_dim=10))\n",
    "model.add(keras.layers.core.Activation(\"relu\"))\n",
    "model.add(keras.layers.core.Dense(output_dim=10))\n",
    "model.add(keras.layers.core.Activation(\"sigmoid\"))\n",
    "model.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\")\n",
    "\n",
    "#randomly generate some inputs\n",
    "rand_inp = np.random.random((10, 100, 4))\n",
    "\n",
    "#confirm that forward and reverse-complement versions give same results\n",
    "fwd_predict = model.predict(rand_inp)\n",
    "rev_predict = model.predict(rand_inp[:, ::-1, ::-1])\n",
    "\n",
    "#print the maximum value of the forward and reverse predictions\n",
    "#should give 0.502919\n",
    "print(\"Max prediction on forward seqs\",np.max(fwd_predict))\n",
    "print(\"Max prediction on revcomps\",np.max(rev_predict))\n",
    "\n",
    "#print the max difference in predictions\n",
    "#should give 0.0\n",
    "print(\"Maximum absolute difference:\",np.max(np.abs(fwd_predict - rev_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
