{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read data as dataframe\n",
    "df = pd.read_csv(\"/home/donajialej/data/3-Simon/promoterDHS_distalDHS_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# Define a function to convert a chromosome string to a genomic sequence\n",
    "def chrom2seq(chrom):\n",
    "    \"\"\"\n",
    "    Convert chromosome string \"chrom\" to a genomic sequence\n",
    "    \"\"\"\n",
    "\n",
    "    return list(SeqIO.parse(\n",
    "        \"/home/donajialej/data/3-Simon/hg38.analysisSet.chroms/%s.fa\" % chrom, \"fasta\"))[0].seq.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom2seq('chr1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of chromosomes\n",
    "CHROMS = [\"chr1\", \"chr10\", \"chr11\", \"chr12\", \"chr13\", \"chr14\", \"chr15\", \"chr16\", \"chr17\", \n",
    "          \"chr18\", \"chr19\", \"chr2\", \"chr20\", \"chr21\", \"chr22\", \"chr3\", \"chr4\", \"chr5\", \"chr6\", \n",
    "          \"chr7\", \"chr8\", \"chr9\", \"chrM\", \"chrX\", \"chrY\"]\n",
    "\n",
    "# Make dictionary\n",
    "CHROM2SEQ = dict(zip(CHROMS,[chrom2seq(chrom) for chrom in CHROMS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = CHROM2SEQ['chr1'][925589:925790]\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to create a one-hot encoding for a genomic sequence\n",
    "base2row = {'A':0,'T':1,'C':2,'G':3}\n",
    "def seq2onehot(seq):\n",
    "    \"\"\"\n",
    "    Create a one-hot encoding of a nucleotide sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.zeros((len(seq),4),dtype=bool)\n",
    "    for i, base in enumerate(seq):\n",
    "        if base=='N':\n",
    "            continue\n",
    "        A[i,base2row[base]] = True\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1h = seq2onehot(seq)\n",
    "np.shape(seq1h)\n",
    "print(seq1h[:9,:])\n",
    "seq[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a sample\n",
    "df = df.sample(n=100000)\n",
    "\n",
    "# Create new columns for promoter length and distal length\n",
    "df[\"promoter_len\"] = df[\"promoterDHSend\"]-df[\"promoterDHSstart\"]\n",
    "df[\"distal_len\"] = df[\"distalDHSend\"]-df[\"distalDHSstart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of sequence lengths\n",
    "plt.figure\n",
    "plt.hist(df[\"promoter_len\"],bins=30,color=\"r\",alpha=0.4,label=\"promoter\")\n",
    "plt.hist(df[\"distal_len\"],bins=30,color=\"b\",alpha=0.4,label=\"distal\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"base pair\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set a maximum sequence length and filter out longer sequences\n",
    "seqlength = 2000\n",
    "df = df[(df[\"promoter_len\"]<seqlength) & (df[\"distal_len\"]<seqlength)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize one-hot encoding arrays (X)\n",
    "N = df.shape[0]\n",
    "encoding_promoter = np.zeros((N,seqlength,4),dtype=bool)\n",
    "encoding_distal = np.zeros((N,seqlength,4),dtype=bool)\n",
    "\n",
    "# Initialize Y\n",
    "Y = np.zeros((N,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define X (one-hot encoding) and Y (linked) for Keras\n",
    "row_no = 0\n",
    "for i,row in df.iterrows():\n",
    "    seq_promoter = CHROM2SEQ[row.chr][row.promoterDHSstart:row.promoterDHSend]\n",
    "    seq_distal = CHROM2SEQ[row.chr][row.distalDHSstart:row.distalDHSend]\n",
    "    try:\n",
    "        encoding_promoter[row_no,:row.promoter_len,:] = seq2onehot(seq_promoter)\n",
    "        encoding_distal[row_no,:row.distal_len,:] = seq2onehot(seq_distal)\n",
    "        Y[row_no] = row.linked\n",
    "        row_no += 1\n",
    "    except KeyError:\n",
    "        print(seq_promoter)\n",
    "        print(seq_distal)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=Y[:,0]\n",
    "print(np.shape(encoding_promoter))\n",
    "print(np.shape(encoding_distal))\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training, development and test data sets\n",
    "\n",
    "train_idx, test_idx, y_train, y_test = train_test_split(\n",
    "    np.arange(N), Y, test_size=0.25)\n",
    "\n",
    "train_idx, dev_idx, y_train, y_dev = train_test_split(\n",
    "    train_idx, y_train, test_size=0.33)\n",
    "\n",
    "xp_train = encoding_promoter[train_idx,:,:]\n",
    "xp_dev = encoding_promoter[dev_idx,:,:]\n",
    "xp_test = encoding_promoter[test_idx,:,:]\n",
    "\n",
    "xd_train = encoding_distal[train_idx,:,:]\n",
    "xd_dev = encoding_distal[dev_idx,:,:]\n",
    "xd_test = encoding_distal[test_idx,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some functions for the model\n",
    "\n",
    "def reluConv1d(x, filters, kernel_size, name):\n",
    "    return Conv1D(filters=filters, kernel_size=kernel_size, \n",
    "                  activation='relu', padding='same', name=name)(x)\n",
    "\n",
    "def gmp1d(x):\n",
    "    return GlobalMaxPooling1D()(x)\n",
    "\n",
    "def convMP(x, filters, kernel_size, name):\n",
    "    return gmp1d(reluConv1d(x, filters, kernel_size, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "onehot = Input(shape=(seqlength,4),name='distalDHSoh')\n",
    "fingerprint = convMP(onehot,12,6,'Conv1DdistalDHS')\n",
    "prob_association = Dense(1,activation='sigmoid',name='logistic_regr')(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=onehot,outputs=prob_association)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the percentage of nonzeros in Y\n",
    "pc = len(np.where(Y)[0]) / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEAJANDRA: Normalized mutual informetion metric to use in keras (Basically copied from SKLEARN)\n",
    "from scipy import sparse as sp\n",
    "from math import log\n",
    "from sklearn.utils.validation import check_array\n",
    "\n",
    "def build_normalized_mutual_info(labels_true, labels_pred):\n",
    "    \"\"\" Evaluate the normalized mutual info from sklearn to used in keras,\n",
    "    but erasing the part where sklear cheack if the arrays are correct sizes (i.e check_clusterings )\n",
    "    \"\"\"\n",
    "    #copy from normalized_mutual_info_score:\n",
    "    classes = np.unique(labels_true)\n",
    "    clusters = np.unique(labels_pred)\n",
    "    # Special limit cases: no clustering since the data is not split.\n",
    "    # This is a perfect match hence return 1.0.\n",
    "    if (classes.shape[0] == clusters.shape[0] == 1 or\n",
    "            classes.shape[0] == clusters.shape[0] == 0):\n",
    "        return 1.0\n",
    "    contingency = build_contingency_matrix(labels_true, labels_pred, sparse=True)\n",
    "    contingency = contingency.astype(np.float64)\n",
    "    contingency = check_array(contingency,\n",
    "                                  accept_sparse=['csr', 'csc', 'coo'],\n",
    "                                  dtype=[int, np.int32, np.int64])\n",
    "    \n",
    "    #copy from mutual_info_score:\n",
    "    if isinstance(contingency, np.ndarray):\n",
    "        # For an array\n",
    "        nzx, nzy = np.nonzero(contingency)\n",
    "        nz_val = contingency[nzx, nzy]\n",
    "    elif sp.issparse(contingency):\n",
    "        # For a sparse matrix\n",
    "        nzx, nzy, nz_val = sp.find(contingency)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported type for 'contingency': %s\" %\n",
    "                         type(contingency))\n",
    "    contingency_sum = contingency.sum()\n",
    "    pi = np.ravel(contingency.sum(axis=1))\n",
    "    pj = np.ravel(contingency.sum(axis=0))\n",
    "    log_contingency_nm = np.log(nz_val)\n",
    "    contingency_nm = nz_val / contingency_sum\n",
    "    # Don't need to calculate the full outer product, just for non-zeroes\n",
    "    outer = pi.take(nzx) * pj.take(nzy)\n",
    "    log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())\n",
    "    mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +\n",
    "          contingency_nm * log_outer)\n",
    "    \n",
    "    # back to noralized_normalized_mutual_info_score:\n",
    "    # Calculate the expected value for the mutual information\n",
    "    # Calculate entropy for each labeling\n",
    "    h_true, h_pred = build_entropy(labels_true), build_entropy(labels_pred)\n",
    "    nmi = mi / max(np.sqrt(h_true * h_pred), 1e-10)\n",
    "    return K.variable(value=nmi[0])\n",
    "\n",
    "def build_contingency_matrix(labels_true, labels_pred, eps=None, sparse=False):\n",
    "    \"\"\"Build a contingency matrix describing the relationship between labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_true : int array, shape = [n_samples]\n",
    "        Ground truth class labels to be used as a reference\n",
    "    labels_pred : array, shape = [n_samples]\n",
    "        Cluster labels to evaluate\n",
    "    eps : None or float, optional.\n",
    "        If a float, that value is added to all values in the contingency\n",
    "        matrix. This helps to stop NaN propagation.\n",
    "        If ``None``, nothing is adjusted.\n",
    "    sparse : boolean, optional.\n",
    "        If True, return a sparse CSR continency matrix. If ``eps is not None``,\n",
    "        and ``sparse is True``, will throw ValueError.\n",
    "        .. versionadded:: 0.18\n",
    "    Returns\n",
    "    -------\n",
    "    contingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]\n",
    "        Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in\n",
    "        true class :math:`i` and in predicted class :math:`j`. If\n",
    "        ``eps is None``, the dtype of this array will be integer. If ``eps`` is\n",
    "        given, the dtype will be float.\n",
    "        Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``.\n",
    "    \"\"\"\n",
    "\n",
    "    if eps is not None and sparse:\n",
    "        raise ValueError(\"Cannot set 'eps' when sparse=True\")\n",
    "\n",
    "    classes, class_idx = np.unique(labels_true, return_inverse=True)\n",
    "    clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)\n",
    "    n_classes = classes.shape[0]\n",
    "    n_clusters = clusters.shape[0]\n",
    "    # Using coo_matrix to accelerate simple histogram calculation,\n",
    "    # i.e. bins are consecutive integers\n",
    "    # Currently, coo_matrix is faster than histogram2d for simple cases\n",
    "    contingency = sp.coo_matrix((np.ones(class_idx.shape[0]),\n",
    "                                 (class_idx, cluster_idx)),\n",
    "                                shape=(n_classes, n_clusters),\n",
    "                                dtype=np.int)\n",
    "    if sparse:\n",
    "        contingency = contingency.tocsr()\n",
    "        contingency.sum_duplicates()\n",
    "    else:\n",
    "        contingency = contingency.toarray()\n",
    "        if eps is not None:\n",
    "            # don't use += as contingency is integer\n",
    "            contingency = contingency + eps\n",
    "    return contingency\n",
    "\n",
    "def build_entropy(labels):\n",
    "    \"\"\"Calculates the entropy for a labeling.\"\"\"\n",
    "    if len(labels) == 0:\n",
    "        return 1.0\n",
    "    label_idx = np.unique(labels, return_inverse=True)[1]\n",
    "    pi = np.bincount(label_idx).astype(np.float64)\n",
    "    pi = pi[pi > 0]\n",
    "    pi_sum = np.sum(pi)\n",
    "    # log(a / b) should be calculated as log(a) - log(b) for\n",
    "    # possible loss of precision\n",
    "    return -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alejandra: kullback leibler divergence copued from old verison of Keras\n",
    "def build_kullback_leibler_divergence(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEJANDRA: F1 metric tu use in keras, basically copied from old version of Keras\n",
    "\n",
    "def build_precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def build_recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def build_fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = build_precision(y_true, y_pred)\n",
    "    r = build_recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def build_fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return build_fbeta_score(y_true, y_pred, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F measure metric\n",
    "\n",
    "Just add in build_fmeasure to your metric options on the model compile (**without ' ' **)\n",
    "\n",
    "## example\n",
    "\n",
    "model.compile(optimizer=Adam(),loss='binary_crossentropy',metrics=[build_fmeasure,'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),loss='binary_crossentropy',metrics=['mse','accuracy',build_normalized_mutual_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit the model using a dictionary to weight class 1 more highly than class 0\n",
    "so that both classes are represented equally in the model fitting\n",
    "\"\"\"\n",
    "class_weight = {0: 1, 1: min([round(1/pc),50])}\n",
    "model.fit(xd_train,y_train,batch_size=256,epochs=1000,\n",
    "          validation_data=[xd_dev,y_dev],class_weight=class_weight,callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = model.get_layer('Conv1DdistalDHS').get_weights()[0]\n",
    "plt.imshow(filters[:,:,8].T)\n",
    "plt.colorbar()\n",
    "filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = model.get_layer('logistic_regr').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualizing the filters\n",
    "\"\"\"\n",
    "fig, axs = plt.subplots(3,4, figsize=(15, 6), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "for i in range(12):\n",
    "    im=axs[i].imshow(filters[:,:,i].T)\n",
    "    plt.colorbar(im,ax=axs[i])\n",
    "plt.title('Filters')\n",
    "plt.show()\n",
    "\n",
    "np.shape(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#y_score = model.predict(xd_test)\n",
    "\n",
    "y_proba = model.predict(xd_test)\n",
    "y_score = np.argmax(y_proba, axis=-1)\n",
    "\n",
    "\"\"\"\n",
    "#ROC score\n",
    "\"\"\"\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')\n",
    "plt.show()\n",
    "print('AUC: %f' % roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(xd_test, y_test)\n",
    "print(score)\n",
    "print(model.metrics_names)\n",
    "\n",
    "pd.DataFrame(data=[score[0:3]], \n",
    "             columns=model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nmi_score=normalized_mutual_info_score(y_test,y_score)\n",
    "\n",
    "print('accuracy score = {}'.format(accuracy_score(y_test, y_score)))\n",
    "print('normalized mutual information = {}'.format(nmi_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_normalized_mutual_info(y_test,y_score))\n",
    "print(normalized_mutual_info_score(y_test,y_score))\n",
    "print(build_kullback_leibler_divergence(y_test,y_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
